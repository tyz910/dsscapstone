---
title: "Data Science Capstone: Milestone Report"
output: 
  html_document:
    keep_md: true
---

## Synopsis

todo

## Loading training data

Download Training data from [Courcera site](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip). Files stored in zip archive, unzip it.
```{r}
loadData <- function () {
  if (!file.exists("./data/Coursera-SwiftKey.zip")) {
    dataUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
    download.file(dataUrl, destfile = "./data/Coursera-SwiftKey.zip", method = "curl")
  }
  
  unzip("./data/Coursera-SwiftKey.zip", exdir = "./data");
}

if (!file.exists("./data")) {
  dir.create("./data")
}

if (!file.exists("./data/final")) {
  loadData()
}
```

There are 4 folders after unzipping.
```{r}
list.files("./data/final")
```

Each folder contains 3 files for specific language.
```{r}
list.files("./data/final/en_US")
```

## Files analysis

We will use english files for analysis. First we read the files.
```{r files, cache=TRUE}
readFiles <- function () {
  files <- c()
  paths <- c(
    "blogs"   = "./data/final/en_US/en_US.blogs.txt",
    "news"    = "./data/final/en_US/en_US.news.txt",
    "twitter" ="./data/final/en_US/en_US.twitter.txt"
  )
  
  for (name in names(paths)) {
    files[[name]] <- readLines(paths[name], encoding = "UTF-8", skipNul = TRUE)
  }
  
  return(files)
}

if (!exists("files")) {
  files <- readFiles()
}
```

Calculating word and line counts.
```{r files_stat, cache=TRUE}
library(stringi)

getFileStats <- function () {
  files_stat <- NULL
  
  for (name in names(files)) {
    file <- files[[name]]
    stats <- stri_stats_general(file)
    data <- data.frame(t(stats), row.names = name)
    data$Words <- sum(stri_count_words(file))

    if (is.null(files_stat)) {
      files_stat <- data
    } else {
      files_stat <- rbind(files_stat, data)
    }
  }
  
  return(files_stat)
}

files_stat <- getFileStats()
files_stat
```

Plotting graphs.
```{r files_stat_graph, cache=TRUE}
barplot(files_stat[, "Lines"], names.arg = row.names(files_stat), main = "Line counts")
barplot(files_stat[, "Words"], names.arg = row.names(files_stat), main = "Word counts")
```

## N-gram analysis

For N-gram analysis we create sample set containts 1000 lines from news, 10000 lines from blogs and 50000 lines from twitter.
```{r corpus, cache=TRUE, message=FALSE, warning=FALSE}
library("tm")
set.seed(123)

createSample <- function (blogs, news, twitter) {
  samples <- c()

  for (name in names(files)) {
    file <- files[[name]]
    samples <- c(samples, sample(file, get(name)))
  }
  
  return(samples)
}

getCorpus <- function (data) {
  corpus <- Corpus(VectorSource(data))
  corpus <- tm_map(corpus, tolower)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)

  return(corpus)
}

getSampleCorpus <- function (blogs = 10000, news = 1000, twitter = 50000, cache = TRUE) {
  cacheFile <- "./data/sample_corpus.RDS"
  if (cache && file.exists(cacheFile)) {
    corpus <- readRDS(cacheFile)
  } else {
    samples <- createSample(blogs, news, twitter)
    corpus <- getCorpus(samples)
    saveRDS(corpus, file = "./data/sample_corpus.RDS")
  }

  return(corpus)
}

corpus <- getSampleCorpus(cache = TRUE)
```

Calculate and plot top 15 tokens from 1-grams to 4-grams.
```{r tokens_plot, cache=TRUE, message=FALSE, warning=FALSE}
library("RWeka")

printNGram <- function (corpus, n = 1, topN = 15, delim = " \\r\\n\\t.!?,;\"()") {
  label <- paste('Top ', topN, ' ', n, '-grams', sep = '')
  token <- NGramTokenizer(corpus$content, Weka_control(min = n, max = n, delimiters = delim))
  top <- as.data.frame(table(token))
  top <- head(top[order(-top$Freq), ], topN)
  
  par(mar = c(5, 8, 2, 1))
  barplot(rev(top$Freq), names.arg = rev(top$token), main = label, xlab = "Frequency", horiz = TRUE, las = 1, cex.names = 0.9)
  
  return(top)
}

printNGrams <- function (corpus, num = 4, topN = 15) {
  for (n in 1:num) { 
    printNGram(corpus, n, topN)
  }
}

printNGrams(corpus)
```

## Plans

todo
